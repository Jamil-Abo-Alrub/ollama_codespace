{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Industrialisation et MLOps sur Microsoft Azure**\n",
    "\n",
    "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8cSAbbthmBHXpIZaYW6hNpMfMkXvhRP7P5w&s' width=500>\n",
    "\n",
    "L'objectif de cette s√©ance est de sortir du cadre local pour b√¢tir une **infrastructure MLOps compl√®te et scalable** sur le cloud Azure. Nous allons passer d'un simple script d'entra√Ænement √† une architecture interconnect√©e capable de g√©rer le cycle de vie d'un mod√®le de bout en bout : stockage, base de donn√©es, suivi d'exp√©riences et calcul haute performance.\n",
    "\n",
    "Au cours de ce notebook, nous allons orchestrer les services suivants :\n",
    "\n",
    "1.  **Calcul et Entra√Ænement (Compute Plane) :** Utilisation d'une **Machine Virtuelle (VM) Azure** d√©di√©e, configur√©e avec Docker, pour lancer des entra√Ænements intensifs qui communiqueront automatiquement leurs r√©sultats vers notre serveur MLflow distant.\n",
    "2.  **Gestion des ressources :** Utilisation de l'**extension Azure pour VS Code** pour cr√©er et monitorer nos services sans quitter notre environnement de d√©veloppement.\n",
    "3.  **Persistance des donn√©es :**\n",
    "    *   D√©ploiement d'une base de donn√©es **PostgreSQL sur Azure** pour stocker les m√©tadonn√©es de MLflow.\n",
    "    *   Configuration d'un **Azure Blob Storage** pour servir de \"Artifact Store\" (stockage des mod√®les et des datasets volumineux).\n",
    "4.  **Azure Web App (for Containers)** : D√©ploiement d'un serveur **MLflow** conteneuris√© via . Ce serveur centralisera tous nos logs d'entra√Ænement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Connexion √† Azure et Installation des outils**\n",
    "\n",
    "Avant de d√©ployer nos services, nous devons pr√©parer notre environnement de d√©veloppement pour qu'il puisse \"parler\" avec l'infrastructure de Microsoft Azure.\n",
    "\n",
    "Vous avez re√ßu un e-mail de la part de Microsoft Azure (v√©rifiez vos courriers ind√©sirables). \n",
    "1. Cliquez sur le lien d'acceptation dans l'e-mail pour rejoindre le **compte de facturation de l'intervenant**.\n",
    "2. Cela vous donnera les droits n√©cessaires pour cr√©er des ressources (Web App, PostgreSQL, VM) sans avoir √† renseigner votre propre carte bancaire.\n",
    "3. Une fois accept√©, connectez-vous au portail [portal.azure.com](https://portal.azure.com).\n",
    "\n",
    "4. Nous allons utiliser Python pour interagir avec le stockage Azure (Blob Storage) et g√©rer le cycle de vie de nos mod√®les avec MLflow. Ex√©cutez la cellule suivante pour installer les d√©pendances n√©cessaires :\n",
    "\n",
    "*   `azure-storage-blob` : Pour envoyer/t√©l√©charger des fichiers (mod√®les, datasets) sur le cloud.\n",
    "*   `psycopg2-binary` : Pour permettre √† MLflow de communiquer avec notre base de donn√©es PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies pour Azure : azure-storage-blob psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Configuration de VS Code\n",
    "L'extension Azure permet de g√©rer vos ressources (d√©marrer/arr√™ter une VM, voir les logs d'une Web App) directement depuis votre √©diteur.\n",
    "\n",
    "6.  **Installation :** Allez dans l'onglet \"Extensions\" de VS Code et installez le pack **[Azure Tools](https://marketplace.visualstudio.com/items?itemName=ms-vscode.vscode-node-azure-pack)**.\n",
    "7.  **Connexion :**\n",
    "    *   Cliquez sur l'ic√¥ne Azure qui vient d'appara√Ætre dans votre barre lat√©rale gauche.\n",
    "    *   Cliquez sur **\"Sign in to Azure...\"**.\n",
    "    *   Une fen√™tre de navigateur s'ouvre : connectez-vous avec le compte sur lequel vous avez re√ßu l'invitation.\n",
    "8.  **S√©lection de l'abonnement :**\n",
    "    *   Une fois connect√©, assurez-vous de s√©lectionner l'abonnement **Ynov 2026** li√© au cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. D√©ploiement d'une application Web (Streamlit)**\n",
    "\n",
    "Avant de d√©ployer des syst√®mes complexes comme MLflow, nous allons commencer par d√©ployer une interface utilisateur simple avec **Streamlit**. Azure App Service (Web App) permet d'h√©berger des applications Python tr√®s facilement.\n",
    "\n",
    "#### **2.1. Pr√©paration des fichiers locaux**\n",
    "\n",
    "Pour qu'Azure sache comment faire tourner votre application, vous avez besoin de deux fichiers : le code (`app.py`) et la liste des d√©pendances (`requirements.txt`).\n",
    "\n",
    "**√âtape 1 : Cr√©er le fichier `app.py`**\n",
    "Dans votre dossier de projet, cr√©ez un fichier `app.py` et collez le code suivant :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Azure Deploy : Succ√®s ! ‚úÖ\")\n",
    "st.write(\"Cette application Streamlit est h√©berg√©e sur Azure Web Apps.\")\n",
    "st.info(\"F√©licitations, vous venez de d√©ployer votre premier service cloud.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un excellent choix. Passer par Docker est la m√©thode la plus robuste (\"Cloud Native\") car elle garantit que votre application fonctionnera exactement de la m√™me mani√®re sur Azure que sur votre PC, sans se soucier des probl√®mes de d√©pendances ou de fichiers manquants.\n",
    "\n",
    "\n",
    "#### **√âtape 2 : Cr√©er le fichier `Dockerfile`**\n",
    "\n",
    "Le `Dockerfile` est la recette de cuisine qui permet de construire l'image de votre application. Cr√©ez un fichier nomm√© exactement `Dockerfile` (sans extension) √† la racine de votre projet :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "FROM python:3.13-slim\n",
    "\n",
    "RUN pip install streamlit\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **√âtape 3 : Construction et publication de l'image**\n",
    "\n",
    "Nous allons maintenant transformer ce fichier en une image et l'envoyer sur **Docker Hub** pour qu'Azure puisse la r√©cup√©rer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion √† Docker Hub\n",
    "docker login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction de l'image\n",
    "docker build -t votre_pseudo/streamlit ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publication de l'image\n",
    "docker push votre_pseudo/streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **√âtape 4 : D√©ploiement sur Azure Web App for Containers**\n",
    "\n",
    "Maintenant que l'image est sur le Cloud (Docker Hub), nous allons dire √† Azure de l'utiliser.\n",
    "\n",
    "1.  **Cr√©ation de la ressource :**\n",
    "    *   Allez sur le [Portail Azure](https://portal.azure.com).\n",
    "    *   Cliquez sur **\"Cr√©er une ressource\"** -> **\"Web App\"**.\n",
    "2.  **Configuration de base :**\n",
    "    *   *Nom :* Un nom unique (ex: `ia-cloud-docker-votre-nom`).\n",
    "    *   *Publier :* S√©lectionnez **Conteneur Docker**.\n",
    "    *   *Syst√®me d'exploitation :* **Linux**.\n",
    "    *   *R√©gion :* **France Central** ou **West Europe**.\n",
    "3.  **Configuration du conteneur (Onglet Docker) :**\n",
    "    *   *Source d'image :* **Docker Hub**.\n",
    "    *   *Type d'acc√®s :* **Public**.\n",
    "    *   *Image et balise :* Tapez `votre_pseudo/streamlit-azure:latest`.\n",
    "4.  **Finalisation :**\n",
    "    *   Cliquez sur **Review + Create** puis **Create**.\n",
    "\n",
    "\n",
    "#### **√âtape 5 : Configuration du Port sur Azure**\n",
    "\n",
    "Azure doit savoir que votre conteneur √©coute sur le port **8501**.\n",
    "\n",
    "1.  Une fois la Web App cr√©√©e, allez dans **Configuration** (menu de gauche).\n",
    "2.  Dans l'onglet **Application settings**, cliquez sur **+ New application setting**.\n",
    "3.  Ajoutez la variable suivante :\n",
    "    *   **Nom :** `WEBSITES_PORT`\n",
    "    *   **Valeur :** `8501`\n",
    "4.  Cliquez sur **OK** puis **Save**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **3 : Azure Blob Storage ‚Äî G√©rer vos donn√©es dans le Cloud**\n",
    "\n",
    "<img src='https://cdn-dynmedia-1.microsoft.com/is/image/microsoftcorp/Introduction-to-Azure-Blob-Storage_tbmnl_en-us?scl=1' width=400>\n",
    "\n",
    "\n",
    "Le **Blob Storage** est le service de stockage d'objets d'Azure (√©quivalent de AWS S3). Il est id√©al pour stocker des datasets, des images, des logs ou des fichiers de mod√®les s√©rialis√©s (`.pkl`, `.h5`, `.onnx`).\n",
    "\n",
    "\n",
    "\n",
    "### **Etape 1. Cr√©ation du stockage via VS Code**\n",
    "\n",
    "Plut√¥t que d'utiliser le portail web, nous allons utiliser l'extension **Azure Tools** :\n",
    "1. Cliquez sur l'ic√¥ne **Azure** dans la barre lat√©rale.\n",
    "2. D√©roulez votre abonnement et cherchez **Storage**.\n",
    "3. Faites un clic droit sur **Storage Accounts** -> **Create Storage Account... (Advanced)**.\n",
    "4. Suivez les √©tapes :\n",
    "   * **Nom :** Un nom unique en minuscules et chiffres uniquement (ex: `ynov-prenom`).\n",
    "   * **Performance :** Standard.\n",
    "   * **Replication :** LRS (le moins cher).\n",
    "5. Une fois cr√©√©, faites un clic droit sur votre nouveau compte de stockage -> **Copy Connection String**. C'est cette cl√© qui nous permettra de nous connecter en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "storage_account_key = \"\"\n",
    "storage_account_name = \"\"\n",
    "connection_string = \"\"\n",
    "container_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Etape 2. Manipulation du stockage avec Python**\n",
    "\n",
    "Nous allons configurer nos acc√®s et d√©finir nos fonctions de base pour l'upload et le download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "\n",
    "# Initialisation du client de service\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Fonction pour uploader un fichier\n",
    "def uploadToBlobStorage(file_path, file_name):\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True) # overwrite=True √©vite les erreurs si le fichier existe d√©j√†\n",
    "        print(f\"‚úÖ Fichier {file_name} upload√© avec succ√®s.\")\n",
    "\n",
    "\n",
    "# Fonction pour t√©l√©charger un fichier\n",
    "def download_blob_to_file(file_name, download_path):\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "    with open(file=download_path, mode=\"wb\") as sample_blob:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        sample_blob.write(download_stream.readall())\n",
    "        print(f\"‚úÖ Fichier {file_name} t√©l√©charg√© vers {download_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fichier requirements.txt upload√© avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "# Test des fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fichier requirements.txt t√©l√©charg√© vers downloaded_requirements.txt.\n"
     ]
    }
   ],
   "source": [
    "# Test des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> √Ä vous de jouer ! Compl√©tez les fonctions suivantes pour automatiser la gestion de vos ressources.\n",
    "Sources = https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-python-get-started?tabs=azure-ad\n",
    "\n",
    "\n",
    "#### **Exercice 1 : Lister les fichiers d'un container**\n",
    "Cr√©ez la fonction qui affiche le nom de chaque fichier (blob) pr√©sent dans votre container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_in_container(container_name):\n",
    "    ...\n",
    "    return files\n",
    "\n",
    "# Test :\n",
    "# mes_fichiers = list_blobs_in_container(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercice 2 : T√©l√©chargement intelligent**\n",
    "Cr√©ez une fonction qui r√©cup√®re la liste des fichiers et t√©l√©charge automatiquement le premier fichier trouv√© qui correspond √† une extension sp√©cifique (ex: `.jpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_first_match(extension):\n",
    "    ...\n",
    "    return download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pU_PLA737Hs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHp5RlAG37Hs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-0iNgUt37Hs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projet : D√©ploiement de MLflow sur Azure**\n",
    "\n",
    "### **1. Le Dockerfile (`mlflow-azure`)**\n",
    "Cr√©ez un `Dockerfile` qui installe MLflow et le connecteur Azure.\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.13-slim\n",
    "\n",
    "RUN pip install mlflow azure-storage-blob psycopg2-binary\n",
    "\n",
    "# Port par d√©faut de MLflow\n",
    "EXPOSE 5000\n",
    "\n",
    "# Commande de lancement (on utilise le Blob Storage pour les artefacts)\n",
    "# Remplacez <votre_container> et <votre_compte> ou utilisez des variables d'env\n",
    "CMD mlflow server \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --artifacts-destination wasbs://${AZURE_CONTAINER}@${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/ \\\n",
    "    --allowed-hosts \"*\" \n",
    "```\n",
    "\n",
    "**Action :** Buildez et pushez l'image :\n",
    "`docker build -t pseudo/mlflow-azure .`  \n",
    "`docker push pseudo/mlflow-azure`\n",
    "\n",
    "\n",
    "### **2. D√©ploiement sur Azure Web App**\n",
    "1.  Cr√©ez une **Web App for Containers** (nom : `ynov-nom-mlflow`).\n",
    "2.  Configurez l'image sur `pseudo/mlflow-azure`.\n",
    "3.  **Variables d'environnement (Configuration) :** Ajoutez ces cl√©s indispensables :\n",
    "    *   `AZURE_STORAGE_CONNECTION_STRING` : Votre cha√Æne de connexion compl√®te.\n",
    "    *   `AZURE_STORAGE_ACCOUNT` : Le nom de votre compte de stockage.\n",
    "    *   `AZURE_CONTAINER` : Le nom de votre blob container.\n",
    "    *   `WEBSITES_PORT` : `5000`.\n",
    "\n",
    "\n",
    "### **3. Test : Loguer un mod√®le √† distance**\n",
    "Une fois que votre URL MLflow est en ligne, utilisez votre conteneur d'entrainement pour v√©rifier que tout se d√©roule correctement et que les fichiers apparaissent sur le blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4 : Machine Virtuelle (VM)**\n",
    "\n",
    "Pourquoi une VM ? Alors que la Web App est parfaite pour l'interface (MLflow), une **VM** est indispensable pour les entra√Ænements longs, gourmands en RAM ou n√©cessitant des **GPUs**.\n",
    "\n",
    "### **Etape 1. Cr√©ation de la VM via VS Code et instalation de docker**\n",
    "1.  Dans l'extension Azure de VS Code, cherchez **Virtual Machines**.\n",
    "2.  Faites un clic droit -> **Create Virtual Machine... (Advanced)**.\n",
    "3.  **Configuration :**\n",
    "    *   **Image :** Ubuntu 22.04 LTS (Standard).\n",
    "    *   **Size :** `Standard_D2s_v3` (ou une taille gratuite/√©co type `B1s`).\n",
    "    *   **Security :** Choisissez **SSH Key** (VS Code la g√©n√©rera pour vous) ou **Password**.\n",
    "    *   **Ports :** Assurez-vous que le port **22 (SSH)** est ouvert.\n",
    "\n",
    "4. Une fois la VM cr√©√©e, faites un clic droit dessus dans VS Code -> **Connect to Host** (ou utilisez le terminal SSH). Une fois \"dans\" la VM, installez Docker avec ces commandes rapides :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt-get update\n",
    "sudo apt-get install -y docker.io\n",
    "sudo systemctl start docker\n",
    "sudo usermod -aG docker $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Etape 3. Lancement de l'entra√Ænement conteneuris√©**\n",
    "Nous allons lancer un conteneur qui va entra√Æner un mod√®le sur la VM et envoyer les r√©sultats √† votre serveur MLflow (Web App).\n",
    "\n",
    "Utilisez l'image docker d'entrainement en utilisant l'url de votre server mlflow pour enregistrer le mod√®le ainsi que les m√©triques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demarrage du conteneur avec la variable d'environnement pour le tracking MLflow\n",
    "docker run -it -e MLFLOW_TRACKING_URI=\"url_appweb_azure_mlflow\" pseudo/mlflow-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Etape 4. √âvolution : Int√©gration de PostgreSQL**\n",
    "Actuellement, votre MLflow stocke les m√©tadonn√©es dans un fichier local (perdu au red√©marrage). Pour un vrai MLflow industriel, il nous faut une **Base de Donn√©es PostgreSQL** pour que les donn√©es soient persistantes.\n",
    "\n",
    "**Modification du Dockerfile (MLflow) :**\n",
    "Pour connecter MLflow √† Postgres, la commande de d√©marrage dans le conteneur doit devenir :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "mlflow server \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --backend-store-uri postgresql://mlflow:Ynov2026@mlflow-ynov.postgres.database.azure.com:5432/postgres?sslmode=require \\\n",
    "    --artifacts-destination wasbs://${AZURE_CONTAINER}@${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/ \\\n",
    "    --allowed-hosts \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projet : Entra√Ænement d'un CNN sur Azure**\n",
    "\n",
    "C'est l'heure de mettre en pratique toute l'architecture que vous avez mont√©e depuis le d√©but. On va sortir des petits mod√®les de test pour s'attaquer √† du Deep Learning un peu plus costaud : la classification d'images (chiens vs chats).\n",
    "\n",
    "L'objectif est simple : votre PC ne doit rien calculer. Tout va se passer dans un conteneur sur votre VM Azure, et les r√©sultats seront envoy√©s en direct sur votre serveur MLflow.\n",
    "\n",
    "### üéØ **Le challenge**\n",
    "Vous devez entra√Æner un r√©seau de neurones √† convolution (CNN) en utilisant le dataset \"Cat-Dog-Dataset\" disponible sur le GitHub du cours. \n",
    "\n",
    "### **Ce que vous devez faire :**\n",
    "\n",
    "**1. Pr√©parer l'image d'entra√Ænement**\n",
    "Vous allez devoir cr√©er un nouveau Dockerfile. Cette image doit √™tre une v√©ritable \"box\" autonome : elle doit contenir Python, TensorFlow, les outils pour communiquer avec Azure et MLflow, mais aussi cloner automatiquement le dataset depuis GitHub. Votre script d'entra√Ænement (fourni plus haut) devra √™tre int√©gr√© √† cette image.\n",
    "\n",
    "**2. Automatiser le tracking**\n",
    "N'oubliez pas d'int√©grer la fonction d'autologging de MLflow dans votre script. On veut que TensorFlow envoie tout seul les courbes d'accuracy et de loss sur votre Web App Azure pendant que la machine travaille.\n",
    "\n",
    "**3. Envoyer l'image sur Docker Hub**\n",
    "Une fois que votre recette (Dockerfile) est pr√™te, buildez votre image localement et poussez-la sur votre compte Docker Hub. C'est ce qui permettra √† votre VM de la r√©cup√©rer en un instant.\n",
    "\n",
    "**4. Allumer le \"muscle\" (La VM)**\n",
    "Connectez-vous √† votre VM Azure via VS Code. C'est ici que le calcul va se faire. Assurez-vous que Docker est bien install√© et pr√™t √† l'emploi.\n",
    "\n",
    "**5. Lancer le calcul**\n",
    "C'est le moment de v√©rit√©. Lancez votre conteneur sur la VM. Attention : vous devrez passer les informations de connexion (l'URL de votre MLflow et vos cl√©s de stockage) sous forme de variables d'environnement au moment du lancement du conteneur. Sans √ßa, votre VM travaillera dans le vide et ne pourra rien sauvegarder !\n",
    "\n",
    "### ‚úÖ **Comment valider votre projet ?**\n",
    "Le travail est consid√©r√© comme r√©ussi si :\n",
    "1. On voit l'entra√Ænement tourner dans les logs de votre VM.\n",
    "2. Un nouveau projet appara√Æt sur votre interface MLflow avec des graphiques qui bougent en temps r√©el.\n",
    "3. Le mod√®le final (le fichier d'entra√Ænement termin√©) est bien pr√©sent dans votre Blob Storage √† la fin du processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Chargement des fichiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Mickevin/Cat-Dog-Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = 'Cat-Dog-Dataset/train/'\n",
    "list_dir = os.listdir(path)\n",
    "\n",
    "\n",
    "# Cr√©ez un DataFrame avec les noms des images et les labels correspondants\n",
    "df = pd.DataFrame(os.listdir('Cat-Dog-Dataset/train'), columns=['filename'])\n",
    "df['label'] = ['1' if 'cat' in name else '0' for name in df['filename']]\n",
    "\n",
    "# Cr√©ez un DataFrame avec les noms des images et les labels correspondants\n",
    "df_val = pd.DataFrame(os.listdir('Cat-Dog-Dataset/validation'), columns=['filename'])\n",
    "df_val['label'] = ['1' if 'cat' in name else '0' for name in df_val['filename']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Conception du mod√®le CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Conception d'un mod√®le de r√©seau de neuronne √† convolution\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(250, 250, 3)),\n",
    "    Dropout(0.4),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du mod√®le\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Affichage de la structure du mod√®le\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Entrainement du mod√®le**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Cr√©ez un g√©n√©rateur d'images √† partir du DataFrame\n",
    "datagen = ImageDataGenerator(rescale=1./255)  # R√©√©chelonne les valeurs des pixels entre 0 et 1\n",
    "\n",
    "# G√©n√©rateur d'images √† partir du DataFrame\n",
    "generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df[:5000],\n",
    "    directory=path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(250, 250),\n",
    "    class_mode='binary',\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "generator_val = datagen.flow_from_dataframe(\n",
    "    dataframe=df_val[:100],\n",
    "    directory='Cat-Dog-Dataset/validation/',\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(250, 250),\n",
    "    class_mode='binary',\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "# Entra√Ænez le mod√®le en utilisant le g√©n√©rateur d'images\n",
    "history = model.fit(generator, epochs=10,validation_data=generator_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
